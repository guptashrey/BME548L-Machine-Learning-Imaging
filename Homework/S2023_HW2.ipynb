{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpOvvdHwuC80"
      },
      "source": [
        "# Homework 2: Coding Part\n",
        "Due: Mar 2, 2023 at 11:00 pm\n",
        "\n",
        "Submit through Gradescope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUhjKBM_F3co"
      },
      "source": [
        "# Before we begin\n",
        "In NumPy, the name of the game is vectorization. Whenever you have matrices and vectors, try to find an expression for them in terms of matrix manipulations. Then you can implement it NumPy so that *all* values in an array are calulated in one go. Then afterward, you can use appropriate functions (we will use `np.mean` a lot for example) to reduce the arrays. **If you can, it is best to avoid for loops and python control flows.** If you scroll down to Linear Classification with Steepest Descent, you will see a worked out example of how you can do this. \n",
        "\n",
        "Why?\n",
        "\n",
        "Because firstly, the reason NumPy exists is that it has optimized these operations in C++ and is orders of magnitude faster than Python for loops. Secondly, while you can get away with using python control statements now (you will get the right answer so that is not a problem), you **will not** be able to use them with Tensorflow easily. So it is best to practice now. \n",
        "\n",
        "Just to be clear, we will not penalize anyone for not using vectorization if they have the right answer. It does remain highly recommended though.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neBGDgspum9R"
      },
      "source": [
        "# Import datasets and extract features (copy over from the last homework)\n",
        "Don't forget to add the bias term to your features.\n",
        "\n",
        "(***2 POINTS*** for the whole section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m7-YVby7ah2S"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shreygupta/opt/anaconda3/lib/python3.9/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import datasets (just run this block)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "# might take a few minutes\n",
        "digit, target = fetch_openml(\"mnist_784\", return_X_y=True, as_frame=False)\n",
        "digit = (digit/255.0).reshape(-1,28,28)\n",
        "target = target.astype(int)\n",
        "for index, (image, label) in enumerate(list(zip(digit, target))[12:22]):\n",
        "    plt.subplot(1, 10, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r)\n",
        "    plt.title('%i' % label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX7-dVNnwjIh"
      },
      "source": [
        "The Fantastic Visualization code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ae8MxoUWah2z"
      },
      "outputs": [],
      "source": [
        "# just run this block\n",
        "import matplotlib.lines as mlines\n",
        "def newline(weight):\n",
        "    # adapted from https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-matplotlib/36479941\n",
        "    if len(weight) == 2:\n",
        "        p1 = weight[0]\n",
        "        p2 = weight[1]\n",
        "        ax = plt.gca()\n",
        "        xmin, xmax = ax.get_xbound()\n",
        "        if(p2 == 0):\n",
        "            xmin = xmax = 0\n",
        "            ymin, ymax = ax.get_ybound()\n",
        "        else:\n",
        "            ymax = -p1/p2*(xmax)\n",
        "            ymin = -p1/p2*(xmin)\n",
        "        l = mlines.Line2D([xmin,xmax], [ymin,ymax], color = \"g\", label = \"Decision\")\n",
        "        ax.add_line(l)\n",
        "       \n",
        "    elif len(weight) == 3:\n",
        "        p1 = weight[0]\n",
        "        p2 = weight[1]\n",
        "        b = weight[2]\n",
        "        ax = plt.gca()\n",
        "        xmin, xmax = ax.get_xbound()\n",
        "        if(p2 == 0):\n",
        "            xmin = xmax = -b/p1\n",
        "            ymin, ymax = ax.get_ybound()\n",
        "        else:\n",
        "            ymax = -p1/p2*(xmax) - b/p2\n",
        "            ymin = -p1/p2*(xmin) - b/p2\n",
        "        l = mlines.Line2D([xmin,xmax], [ymin,ymax], color = \"g\", label = \"Decision\")\n",
        "        ax.add_line(l)\n",
        "       \n",
        "    return l\n",
        "\n",
        "def visualize_model(features, labels, weights):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    X_features_a = features[np.where(labels==-1)]\n",
        "    X_features_b = features[np.where(labels==1)]\n",
        "    plt.plot(X_features_a[:, 0], X_features_a[:, 1], '.', label = \"{}\".format(number_a))\n",
        "    plt.plot(X_features_b[:, 0], X_features_b[:, 1], '.', label = \"{}\".format(number_b))\n",
        "    plt.xlabel('feature_a')\n",
        "    plt.ylabel('feature_b')\n",
        "    plt.ylim(features[:, 1].min(), features[:, 1].max())\n",
        "    newline(weights)\n",
        "    plt.gca().legend(loc = 1)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1kIx-nRDkn4"
      },
      "source": [
        "Select your champions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4q2uNiJah2Y"
      },
      "outputs": [],
      "source": [
        "# in this block, you only need to modify number_a and number_b (if you wish to)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "number_a = # TODO choose your digit\n",
        "number_b = # TODO choose your digit\n",
        "\n",
        "digit_a_indexes = np.where(target==number_a)[0][:300] # to limit data\n",
        "digit_b_indexes = np.where(target==number_b)[0][:300] # take only 300\n",
        "\n",
        "targets = np.concatenate((target[digit_a_indexes], target[digit_b_indexes]))\n",
        "print(targets.shape[0])\n",
        "images = np.concatenate((digit[digit_a_indexes], digit[digit_b_indexes]))\n",
        "print(images.shape[0])\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, targets, test_size=0.1, random_state=42)\n",
        "for index, image in enumerate(X_train[:10]):\n",
        "    plt.subplot(1, 10, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8YnlU8gDq4A"
      },
      "source": [
        "Add your trusty feature extractors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydyTije6f-pE"
      },
      "outputs": [],
      "source": [
        "# please complete this function:\n",
        "def compute_features(vector):\n",
        "    image = vector.reshape(28, 28) # get back original image shape\n",
        "    def compute_feature_a(image):\n",
        "        '''compute_feature_a will compute ...'''\n",
        "\n",
        "        return\n",
        "\n",
        "    def compute_feature_b(image):\n",
        "        '''compute_feature_b will compute ...'''\n",
        "\n",
        "        return\n",
        "    \n",
        "    return compute_feature_a(image), compute_feature_b(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSvyuJdZah2i"
      },
      "outputs": [],
      "source": [
        "# Apply and plot your features (you can just run this block and inspect the output)\n",
        "\n",
        "X_features = np.apply_along_axis(compute_features, 1,\n",
        "                                 X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
        "\n",
        "X_features_a = X_features[np.where(y_train==number_a)]\n",
        "X_features_b = X_features[np.where(y_train==number_b)]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(X_features_a[:, 0], X_features_a[:, 1], '.')\n",
        "plt.plot(X_features_b[:, 0], X_features_b[:, 1], '.')\n",
        "plt.xlabel('feature_a')\n",
        "\n",
        "plt.ylabel('feature_b')\n",
        "plt.ylim(X_features[:, 1].min(), X_features[:, 1].max()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX7xJP78DzyV"
      },
      "source": [
        "Don't forget those bias terms.\n",
        "\n",
        "Add them to both - your training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9SCbLUzwJrH"
      },
      "outputs": [],
      "source": [
        "X_features = np.apply_along_axis(compute_features, 1,\n",
        "                                 X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
        "\n",
        "X_features_test =  np.apply_along_axis(compute_features, 1,\n",
        "                                 X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
        "\n",
        "# add a bias term to the features\n",
        "\n",
        "# convert training and testing labels to -1 and 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ9bjNaRET3F"
      },
      "source": [
        "*Now* we can have some fun."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4IJl8sNEcQn"
      },
      "source": [
        "# Linear Classification with Finite Differences\n",
        "\n",
        "Rather than use pseudo inverse (analytically solving your optimization equation), we can use gradient descent methods to solve our optimization problem. In gradient descent, we see how the loss function changes when we change the weights and move 'downhill' to find the optimal weight for our model.\n",
        "\n",
        "How do we know where is downhill? We take the gradient of the loss function with respect the to current weight vector. That gives the direction of the change. We then move opposite to that direction ('downhill'). We can numaerically calculate this gradient. Recall that a derivative of a continuous function at some point $x$ is given as: $$ \\lim_{δ\\to 0} \\frac{f(x+δ) - f(x-\\delta)}{2δ}$$\n",
        "\n",
        "We can make use of this.\n",
        "\n",
        "Concretely, start by initializing your weights parameter `w` to some random numbers. Move `w` some small `delta` amount in the each direction to compute the change in your loss and calculate the gradient. Then update `w` by taking a small step in that gradient direction. You can execute this in a for loop for a set number of iterations, or until the gradient reaches some threshold value of not changing very much.\n",
        "\n",
        "This method of perturbing the parameters slightly and taking the difference to calculate the gradient is called the method of Finite Differences.\n",
        "\n",
        "1. Solve for `w`\n",
        "2. Get training and test accuracy every $n$ steps\n",
        "3. Plot the decision boundary every $n$ steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjsC_NZxHiJj"
      },
      "source": [
        "So, first, implement the loss function. For linear classification, it is the mean squared error. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku_IsimAEcQo"
      },
      "outputs": [],
      "source": [
        "# 4 POINTS for correct implementation\n",
        "def loss(features, labels, weights):\n",
        "    return np.inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ousrcBQsEcQp"
      },
      "source": [
        "### Now iteratively solve for your weights `w`, which includes the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd9KGlBYEcQp"
      },
      "outputs": [],
      "source": [
        "# 8 POINTS for correct implementation\n",
        "w = np.random.rand(3) # init weights to some random value.\n",
        "num_iterations = 0 # set this to the number of iterations\n",
        "delta = 1e-5 # This may need to be adjusted\n",
        "step_size = 1e-4 # This may also need to be adjusted\n",
        "start_t = time()\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # move w0 some delta and calculate derivative in w0 direction\n",
        "    # move w1 some delta and calculate derivative in w1 direction\n",
        "    # move w2 some delta and calculate derivative in w2 direction\n",
        "    # these three numbers together form the gradient of loss at w\n",
        "    # update w by taking some step_size in the direction of the gradient going down\n",
        "    # plot the decision boundary and report accuracies every 20 iterations\n",
        "\n",
        "end_t = time()\n",
        "# print final accuracies and plot the decision boundary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_lJcztjJckA"
      },
      "outputs": [],
      "source": [
        "print(\"average time per iteration: {} seconds\".format((end_t-start_t)/num_iterations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfvD0at0ah3E"
      },
      "source": [
        "# Logistic Classification with Finite Differences\n",
        "\n",
        "Now, let's consider the same problem with a different loss function. From problem 2 in the written part, we know that the loss function for a logistic model is:\n",
        "\n",
        "$$L = \\frac{1}{N}\\sum_{i=1}^n ln(1 + e^{-y^{(i)}W^{T}x^{(i)}})$$\n",
        "\n",
        "Use the finite differences method from above to find the optimal `w` for this loss function.\n",
        "\n",
        "1. Solve for `w`\n",
        "2. Get training and test accuracy every $n$ steps\n",
        "3. Plot the decision boundary every $n$ steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH5yaFWKah3G"
      },
      "outputs": [],
      "source": [
        "# 6 POINTS for correct implementation\n",
        "def logistic_loss(features, labels, weights):\n",
        "    return np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If0Mr8VIKxq3"
      },
      "outputs": [],
      "source": [
        "# 6 POINTS for correct implementation\n",
        "w = np.random.rand(3) # init weights to some random value.\n",
        "num_iterations = 0 # set this to the number of iterations\n",
        "delta = 1e-5 # This may need to be adjusted\n",
        "step_size = 1e-4 # This may also need to be adjusted\n",
        "# you might need step_size > 1 as we are taking a mean in this loss\n",
        "start_t = time()\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # move w0 some delta and calculate derivative in w0 direction\n",
        "    # move w1 some delta and calculate derivative in w1 direction\n",
        "    # move w2 some delta and calculate derivative in w2 direction\n",
        "    # these three numbers together form the gradient of loss at w\n",
        "    # update w by taking some step_size in the direction of the gradient going down\n",
        "    # plot the decision boundary and report accuracies every 20 iterations\n",
        "\n",
        "end_t = time()\n",
        "# print final accuracies and plot the decision boundary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFj_qCuzKxrF"
      },
      "outputs": [],
      "source": [
        "print(\"average time per iteration: {} seconds\".format((end_t-start_t)/num_iterations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPhVLeWrah2-"
      },
      "source": [
        "# Linear Classification with Steepest Descent\n",
        "\n",
        "How do we make the optimization go faster? \n",
        "\n",
        "Right now, we numerically estimate the gradient by perturbing weight values and taking the difference in losses. What that means is that we need two forward passes for every model parameter (so total of 6 passes) to make one weight update. Imagine doing this for a neural network that has millions of parameters. We will need to perturb every one of those values, find the difference in the loss values to estimate gradient, then update the weights (and repeat). But we could actually just find an expression for the gradient analytically, and use that at each step directly instead of estimating the gradient numerically.\n",
        "\n",
        "So with that in mind, find the gradient of the linear loss function analytically and code it up. Iniitialize your weights parameter `w` to some random numbers. Using your function, find gradient of the loss the function for the current value of the parameter `w` and then update `w` by taking a small step in direction of the steepest gradient. You can execute this in a for loop for a set number of iterations, or until the gradient reaches some threshold value of not changing very much.\n",
        "\n",
        "1. Solve for `w`\n",
        "2. Get training and test accuracy every $n$ steps\n",
        "3. Plot the decision boundary every $n$ steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GFQfKGXxdqF"
      },
      "source": [
        "## Define the gradient\n",
        "\n",
        "**The following example has been worked out for you.**\n",
        "\n",
        "The loss $L$ we would like to minimize for each $i$ row in our matrix is.\n",
        "\n",
        "$$L  = \\frac{1}{2}\\Sigma (y^{(i)} - W^{T}x^{(i)})^{2}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$x^{(i)} = \n",
        "\\begin{bmatrix}\n",
        "    feature_a \\\\\n",
        "    feature_b \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$W = \n",
        "\\begin{bmatrix}\n",
        "    w_1 \\\\\n",
        "    w_2 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We need to find the gradient of this loss. Let us vectorize the loss. It basically converts the individual operations into a matrix operation, such that we perform the operation on all datapoints at once.\n",
        "\n",
        "In matrix form we convert $x^{(i)}$ into $X$ which is the number of examples we have, in our case is MNIST digit features extracted.\n",
        "\n",
        "$$X = \n",
        "\\begin{bmatrix}\n",
        "    feature_{a1} & feature_{b1} \\\\\n",
        "    feature_{a2} & feature_{b2} \\\\\n",
        "    ... & ... \\\\\n",
        "    feature_{ai} & feature_{bi} \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "and the labels become a vector (in this case a vector of the 'pseudo' labels we have 1, and -1)\n",
        "\n",
        "$$Y = \n",
        "\\begin{bmatrix}\n",
        "    label_{1} \\\\\n",
        "    label_{2} \\\\\n",
        "    ... \\\\\n",
        "    label_{i} \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Then,\n",
        "\n",
        "$$L  = \\frac{1}{2}( Y-XW )^{T} (Y-XW)$$\n",
        "\n",
        "Analytically, we can find that $$ \\frac{d}{dW}L =  - X^{T}Y + X^{T}XW  $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vDMQ3ARx5_8"
      },
      "outputs": [],
      "source": [
        "def linear_gradient(features, labels, weights):\n",
        "  # finds the gradient of the loss function wrt the weight vector\n",
        "  first_term = -features.T@labels\n",
        "  second_term = features.T@features@weights\n",
        "  return first_term + second_term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPlrP_5ah3B"
      },
      "source": [
        "### Now iteratively solve for your weights `w`, which includes the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QH0FXr6ah3C"
      },
      "outputs": [],
      "source": [
        "# 10 POINTS for correct implementation\n",
        "w = np.random.rand(3) # init weights to some random value.\n",
        "num_iterations = 0 # set this to the number of iterations\n",
        "step_size = 1e-3 # This may also need to be adjusted\n",
        "start_t = time()\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # calculate the gradient at the current weight vector\n",
        "    # update w some step_size in the direction of steepest descent\n",
        "    # make plots every 20 iters\n",
        "    \n",
        "end_t = time()\n",
        "# print final accuracies and plot the decision boundary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W43N0CSJFh03"
      },
      "outputs": [],
      "source": [
        "print(\"average time per iteration: {} seconds\".format((end_t-start_t)/num_iterations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqX_v-LXNbyT"
      },
      "source": [
        "# Logistic Classification with Steepest Descent\n",
        "\n",
        "Now we repeat the same process with the **Logistic loss function**. You have (hopefully) analytically calculated the gradient of the Logistic Loss in Problem 2 of the written part. Implement it as a function.\n",
        "\n",
        "Iniitialize your weights parameter `w` to some random numbers. Using your function, find gradient of the loss the function for the current value of the parameter `w` and then update `w` by taking a small step in direction of the steepest gradient. You can execute this in a for loop for a set number of iterations, or until the gradient reaches some threshold value of not changing very much.\n",
        "\n",
        "1. Solve for `w`\n",
        "2. Get training and test accuracy every $n$ steps\n",
        "3. Plot the decision boundary every $n$ steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLRKXYbhNtDz"
      },
      "source": [
        "## Define the gradient\n",
        "Logistic gradient is given as:\n",
        "\n",
        "\\<insert the equation here\\>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensGW1D2eUDw"
      },
      "outputs": [],
      "source": [
        "# 8 POINTS for correct implementation\n",
        "def logistic_gradient(features, labels, weights):\n",
        "    return np.inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULl2-FEUOVcb"
      },
      "source": [
        "### Now iteratively solve for your weights `w`, which includes the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSaEP13JOVcb"
      },
      "outputs": [],
      "source": [
        "# 4 POINTS for correct implementation\n",
        "w = np.random.rand(3) # init weights to some random value.\n",
        "num_iterations = 0 # set this to the number of iterations\n",
        "step_size = 1e-3 # This may also need to be adjusted\n",
        "start_t = time()\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # calculate the gradient at the current weight vector\n",
        "    # update w some step_size in the direction of steepest descent\n",
        "    # make plots every 20 iters\n",
        "    \n",
        "end_t = time()\n",
        "# print final accuracies and plot the decision boundary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVty552rOVcc"
      },
      "outputs": [],
      "source": [
        "print(\"average time per iteration: {} seconds\".format((end_t-start_t)/num_iterations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYCUeD1HOXR7"
      },
      "source": [
        "Was Steepest Descent a faster approach? Since visualization takes a long time, commenting it out from the loop will improve your estimate for time taken. Also if you use the %timeit magic, you will get a better estimate. (2 ***POINTS*** for a short analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJWCZYUCKRp0"
      },
      "source": [
        "\\<Your Answer here>\n",
        "\n",
        "Feel free to use up more space or code blocks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "f5927a6a883e6195d725bb0f0810aab9af9db5b6b29f229ae16434ab81ada32c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
