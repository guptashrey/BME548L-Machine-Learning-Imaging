{"cells":[{"cell_type":"markdown","metadata":{"id":"pjfXsS7SnWt0"},"source":["# TensorFlow deep dive\n","This week we will going inside the workings of Tensorflow to understand some of the implementation details as well as some more adavanced techniques. Conceptual understanding of how Tensorflow works can help in debugging your code."]},{"cell_type":"markdown","metadata":{"id":"Shgt9f81l8Jn"},"source":["##⏰ Exercise\n","There may be bugs in this notebook. Please find and fix them!"]},{"cell_type":"markdown","metadata":{"id":"qoWhziENm8Q4"},"source":["## Resources\n","1. [CSC321 slides](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n","2. [Custom Training walkthrough](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#tensorflow_programming)\n","3. [The Functional API](https://keras.io/guides/functional_api/)\n","4. [Writing your own callback](https://keras.io/guides/writing_your_own_callbacks/)\n","5. [Amazing autodiff explainer](https://www.youtube.com/watch?v=wG_nF1awSSY)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9452,"status":"ok","timestamp":1677549691392,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"BPOQhsKnnS1M"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"2tcBGXRonS1N"},"source":["## Tensors\n","\n","TensorFlow is an infrastructure layer for differentiable programming.\n","At its heart, it's a framework for manipulating N-dimensional arrays (tensors),\n","much like NumPy.\n","\n","However, there are three key differences between NumPy and TensorFlow:\n","\n","- TensorFlow can leverage hardware accelerators such as GPUs and TPUs.\n","- TensorFlow can automatically compute the gradient of arbitrary differentiable tensor expressions.\n","- TensorFlow computation can be distributed to large numbers of devices on a single machine, and large number of\n","machines (potentially with multiple devices each).\n","\n","Let's take a look at the object that is at the core of TensorFlow: the Tensor.\n","\n","Here's a constant tensor:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1677549691634,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"3b6ZOz-knS1N","outputId":"31f37709-b0cd-4848-bc0c-97c9c5c084f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Metal device set to: tf.Tensor(\n","[[5 2]\n"," [1 3]], shape=(2, 2), dtype=int32)\n","Apple M1 Pro\n","\n","systemMemory: 16.00 GB\n","maxCacheSize: 5.33 GB\n","\n"]},{"name":"stderr","output_type":"stream","text":["2023-02-28 20:57:47.150188: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2023-02-28 20:57:47.150354: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["import tensorflow as tf\n","x = tf.constant([[5, 2], [1, 3]])\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"Xj9KuGqgnS1O"},"source":["You can get its value as a NumPy array by calling `.numpy()`:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1677549691898,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"jSa4AttBnS1O","outputId":"38d42e87-b753-47d2-b38f-838abd6f080f"},"outputs":[{"data":{"text/plain":["array([[5, 2],\n","       [1, 3]], dtype=int32)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["x.numpy()"]},{"cell_type":"markdown","metadata":{"id":"geeUGG-mnS1O"},"source":["Much like a NumPy array, it features the attributes `dtype` and `shape`:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1677549691898,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"v7sW7BjdnS1P","outputId":"651d19b5-918e-443d-a252-faeb9361ea46"},"outputs":[{"name":"stdout","output_type":"stream","text":["dtype: <dtype: 'int32'>\n","shape: (2, 2)\n"]}],"source":["print(\"dtype:\", x.dtype)\n","print(\"shape:\", x.shape)"]},{"cell_type":"markdown","metadata":{"id":"Z-LbF4t7nS1P"},"source":["A common way to create constant tensors is via `tf.ones` and `tf.zeros` (just like `np.ones` and `np.zeros`):"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1677549691898,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"a5kD2bsynS1P","outputId":"35cd3351-51f6-4439-dfdd-18db164e27b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[1.]\n"," [1.]], shape=(2, 1), dtype=float32)\n","tf.Tensor(\n","[[0.]\n"," [0.]], shape=(2, 1), dtype=float32)\n"]}],"source":["print(tf.ones(shape=(2, 1)))\n","print(tf.zeros(shape=(2, 1)))"]},{"cell_type":"markdown","metadata":{"id":"2DbvXcMonS1P"},"source":["You can also create random constant tensors:"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677549691898,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"wq1xnSgUnS1P"},"outputs":[],"source":["x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n","\n","x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n"]},{"cell_type":"markdown","metadata":{"id":"wh1byDJDnS1Q"},"source":["## Variables\n","\n","Variables are special tensors used to store mutable state. When something is mutable, it means that the value of that object can be changed or updated. So,we will use Variables for things such as the weights of a neural network.\n","\n","You create a `Variable` using some initial value:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677549691899,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"4kErqVWRnS1Q","outputId":"6837d8c7-0414-40fb-d728-3175c30f5cd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n","array([[ 0.27704275, -0.08301634],\n","       [-1.4391432 ,  0.8447572 ]], dtype=float32)>\n"]}],"source":["initial_value = tf.random.normal(shape=(2, 2))\n","a = tf.Variable(initial_value)\n","print(a)\n"]},{"cell_type":"markdown","metadata":{"id":"4OGdBvuVnS1Q"},"source":["You update the value of a `Variable` by using the methods `.assign(value)`, `.assign_add(increment)`, or `.assign_sub(decrement)`:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1677549692126,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"biRB9SAgnS1Q","outputId":"6fabe959-a6ec-4e65-971d-1065239fa081"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n","array([[-1.9874213 , -0.8751453 ],\n","       [ 0.64128494, -0.03301785]], dtype=float32)>\n","tf.Tensor(\n","[[-1.0088416   0.85940045]\n"," [-1.4712318  -0.08714572]], shape=(2, 2), dtype=float32)\n","<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n","array([[-2.996263  , -0.01574486],\n","       [-0.8299469 , -0.12016356]], dtype=float32)>\n"]}],"source":["new_value = tf.random.normal(shape=(2, 2))\n","a.assign(new_value)\n","print(a)\n","\n","for i in range(2):\n","    for j in range(2):\n","        assert a[i, j] == new_value[i, j]\n","\n","added_value = tf.random.normal(shape=(2, 2))\n","a.assign_add(added_value)\n","print(added_value)\n","print(a)\n","for i in range(2):\n","    for j in range(2):\n","        assert a[i, j] == new_value[i, j] + added_value[i, j]"]},{"cell_type":"markdown","metadata":{"id":"7vaOnFCCsluX"},"source":["If you do not use the assign method, the `Variable` will not remain a `Variable`."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":180,"status":"ok","timestamp":1677549692300,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"wI2RNDsJs9TR","outputId":"51c52410-9492-4c89-8867-2c6924a3ac5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n","array([[-2.996263  , -0.01574486],\n","       [-0.8299469 , -0.12016356]], dtype=float32)>\n","tf.Tensor(\n","[[-4.043505    0.08606432]\n"," [-1.7484736  -1.8703641 ]], shape=(2, 2), dtype=float32)\n"]}],"source":["# a is Variable from above\n","print(a) # you can see it is a variable\n","\n","# let's say we want to add a tensor to a\n","addition = tf.random.normal(shape=(2, 2))\n","\n","# let's see what happens if we don't use the assign method\n","a = a + addition\n","print(a)\n","\n","# it converts to a constant tensor!\n","# Using gradient tape introduced below will not track this tensor\n","# using a Variable directly like this at any point will cause similar behavior\n","# Once a Variable is 'evaluated' to a tensor, you will have re-run your\n","# initialization to get the Variable back."]},{"cell_type":"markdown","metadata":{"id":"hMGsf7xonS1Q"},"source":["## Doing math in TensorFlow\n","\n","If you've used NumPy, doing math in TensorFlow will look very familiar.\n","The main difference is that your TensorFlow code can run on GPU and TPU."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176,"status":"ok","timestamp":1677549692300,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"JI-ND7eynS1Q","outputId":"a98a5838-f6ed-496e-ed0c-4a393f3e29d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[0.560065  0.9603297]\n"," [0.8685211 0.7212865]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[ 0.15118583  0.7084114 ]\n"," [-0.2613254   0.02228578]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[0.7112508  1.6687411 ]\n"," [0.60719573 0.74357224]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[0.5058777  2.7846968 ]\n"," [0.36868665 0.55289966]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[ 1.6584404 16.194908 ]\n"," [ 1.4458344  1.7382861]], shape=(2, 2), dtype=float32)\n"]}],"source":["a = tf.random.normal(shape=(2, 2))\n","b = tf.random.normal(shape=(2, 2))\n","\n","c = a + b\n","d = tf.square(c)\n","e = tf.exp(d)\n","\n","print(a)\n","print(b)\n","print(c)\n","print(d)\n","print(e)"]},{"cell_type":"markdown","metadata":{"id":"I9DF5R7onS1R"},"source":["## Gradients\n","\n","Here's another big difference with NumPy: you can automatically retrieve the gradient of any differentiable expression. \n","\n","Tensorflow using automatic differentiation to find gradients of expressions that use tensors. Specifically, it uses a reverse automatic differentiation. For this, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients. Please explore autodiff more through the resources provided.\n","\n","*General Idea for Implementation:*\n","- Create a \"tape\" data structure that tracks the operations performed in computing a function\n","- Overload primitives to:\n","  - Add themselves to the tape when called\n","  - Compute gradients with respect to their local inputs\n","- Forward pass computes the function, and adds operations to the tape\n","- Reverse pass accumulates the local gradients using the chain rule\n","\n","In practive you can just open a `GradientTape`, start \"watching\" a tensor via `tape.watch()`,\n","and compose a differentiable expression using this tensor as input:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1677549692301,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"pBncXroenS1R","outputId":"ae87ade2-5a15-493a-ffa6-193a964ca10a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n","tf.Tensor([[6. 6.]], shape=(1, 2), dtype=float32)\n","108\n","32\n","tf.Tensor([16. 81.], shape=(2,), dtype=float32)\n","tf.Tensor([ 32. 108.], shape=(2,), dtype=float32)\n"]}],"source":["a = 5*tf.ones(shape=(2, 2))\n","a = tf.constant([[1,2],[3,4]], dtype='float32')\n","a = tf.Variable([2,3], dtype='float32')\n","b = 6*tf.ones(shape=(1, 2))\n","\n","print(a)\n","print(b)\n","\n","print(4*3*3*3)\n","print(4*2*2*2)\n","\n","\n","with tf.GradientTape() as tape:\n","    tape.watch(a)  # Start recording the history of operations applied to `a`\n","    #c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n","    c = tf.square(a) \n","    c = tf.square(c) \n","    print(c)\n","    # What's the gradient of `c` with respect to `a`?\n","    dc_da = tape.gradient(c, a)\n","    print(dc_da)\n"]},{"cell_type":"markdown","metadata":{"id":"hB75kLLMnS1R"},"source":["By default, variables are watched automatically, so you don't need to manually `watch` them:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1677549692521,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"5zFwG0RCnS1R","outputId":"030f7aa9-4717-4184-dd5c-8cdb8d16b9b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor([0.31622776 0.44721356], shape=(2,), dtype=float32)\n"]}],"source":["a = tf.Variable(a)\n","\n","with tf.GradientTape() as tape:\n","    c = tf.sqrt(tf.square(a) + tf.square(b))\n","    dc_da = tape.gradient(c, a)\n","    print(dc_da)"]},{"cell_type":"markdown","metadata":{"id":"f6Gx6LS714zR"},"source":["### Controlling what the tape watches"]},{"cell_type":"markdown","metadata":{"id":"N4VlqKFzzGaC"},"source":["The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n","\n","* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n","* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n","* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n","\n","For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1677549692522,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"Kj9gPckdB37a","outputId":"321434ca-97c4-4420-bef1-db17f57209b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(35.999992, shape=(), dtype=float32)\n","tf.Tensor(5.9999995, shape=(), dtype=float32)\n","None\n","None\n","None\n","tf.Tensor(5.9999995, shape=(), dtype=float32)\n"]}],"source":["# A trainable variable\n","x0 = tf.Variable(3.0)\n","# Not trainable\n","x1 = tf.Variable(3.0, name='x1', trainable=False)\n","# Not a Variable: A variable + tensor returns a tensor.\n","x2 = tf.Variable(2.0, name='x2') + 1.0\n","# Not a variable\n","x3 = tf.constant(3.0, name='x3')\n","\n","with tf.GradientTape() as tape:\n","  y = (x0**2) + (x1**2) + (x2**2) + (x3**2)\n","\n","print(y)\n","\n","grad = tape.gradient(y, [x0, x1, x2, x3])\n","\n","\n","for g in grad:\n","  print(g)\n","\n","\n","# but this works:\n","with tf.GradientTape() as tape:\n","  y2 = (x0**2)\n","\n","grad2 = tape.gradient(y2, x0)\n","\n","print(grad2)"]},{"cell_type":"markdown","metadata":{"id":"RkcpQnLgNxgi"},"source":["You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677549692522,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"hwNwjW1eAkib","outputId":"a8626c3d-8dac-45af-8fe3-845df95eddb4"},"outputs":[{"data":{"text/plain":["['Variable:0']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["[var.name for var in tape.watched_variables()]"]},{"cell_type":"markdown","metadata":{"id":"HZ2LvHifEMgO"},"source":["### Gradients with respect to a model\n","\n","In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":173,"status":"ok","timestamp":1677549692692,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"JvesHtbQESc-"},"outputs":[],"source":["layer = tf.keras.layers.Dense(2, activation='relu')\n","x = tf.constant([[1., 2., 3.]])\n","\n","with tf.GradientTape() as tape:\n","  # Forward pass\n","  y = layer(x)\n","  loss = tf.reduce_mean(y**2)\n","\n","# Calculate gradients with respect to every trainable variable\n","grad = tape.gradient(loss, layer.trainable_variables)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677549692692,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"PR_ezr6UFrpI","outputId":"df1133b7-2671-4904-c8df-608b282d97c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["dense/kernel:0, shape: (3, 2)\n","dense/bias:0, shape: (2,)\n"]}],"source":["for var, g in zip(layer.trainable_variables, grad):\n","  print(f'{var.name}, shape: {g.shape}')"]},{"cell_type":"markdown","metadata":{"id":"-sutlHvzDv72"},"source":["## Advanced Autodiff"]},{"cell_type":"markdown","metadata":{"id":"bbikRGWf6PBJ"},"source":["An autodiff system will convert our expression into a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). These elementary operations and fucntions are called primitives. Primitives have predefined routines to calculate their gradients.\n","\n","This sequence of operations is converted into a computational graph. A computational graph simply turns each operation into a node and connects them through lines, called edges.\n","Say we want to write a function to compute the gradient of the *sigmoid function*:\n","$$\n","\\sigma(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","We can write $\\sigma(x)$ as a composition of several elementary functions, as $\\sigma(x) = s(c(b(a(x))))$, where:\n","\n","$$\n","a(x) = -x\n","$$\n","\n","$$\n","b(a) = e^a\n","$$\n","\n","$$\n","c(b) = 1 + b\n","$$\n","\n","$$\n","s(c) = \\frac{1}{c}\n","$$\n","The computation graph for this expression is shown in the figure below. \n","\n","![graph.png](data:image/png;base64,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)\n","\n","Here, we have \"staged\" the computation such that it contains several intermediate variables, each of which are basic expressions for which we can easily compute the local gradients.\n","\n","The input to this function is $x$, and the output is represented by node $s$. We wish compute the gradient of $s$ with respect to $x$, $\\frac{\\partial s}{\\partial x}$. In order to make use of our intermediate computations, we can use the chain rule as follows:\n","$$\n","\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x}\n","$$\n","\n","In reverse autodiff, to calculate gradient with respect to an input, we start at the Result and traverse the graph backward to our input. At each node, we will encounter a primitive function. We will calculate the derivative of each primitive along the way and multiply them all according to the chain rule to get the final value of the gradient.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677549692692,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"iNoUgnfz_O2a","outputId":"128cf804-1e1c-4c42-97c5-1c9467f00651"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(0.10499353, shape=(), dtype=float32)\n","0.1049935854035065\n"]}],"source":["def grad_sigmoid_manual(x):\n","    \"\"\"Implements the gradient of the logistic sigmoid\n","    function using staged computation\n","    \"\"\"\n","    # Forward pass, keeping track of intermediate values for use in the \n","    # backward pass\n","    a = -x         # -x in denominator\n","    b = np.exp(a)  # e^{-x} in denominator\n","    c = 1 + b      # 1 + e^{-x} in denominator\n","    s = 1.0 / c    # Final result, 1.0 / (1 + e^{-x})\n","    \n","    # Backward pass\n","    dsdc = (-1.0 / (c**2))\n","    dsdb = dsdc * 1\n","    dsda = dsdb * np.exp(a)\n","    dsdx = dsda * (-1)\n","    \n","    return dsdx\n","\n","# using gradient tape instead\n","def grad_sigmoid_automatic(x):\n","  a = tf.constant(x)\n","  with tf.GradientTape() as tape:\n","      tape.watch(a)  # Start recording the history of operations applied to `a`\n","      c = tf.math.sigmoid(a)\n","      # What's the gradient of `c` with respect to `a`?\n","      dc_da = tape.gradient(c, a)\n","  return dc_da\n","\n","# Compare the results of manual and automatic gradient functions:\n","print(grad_sigmoid_automatic(2.0))\n","print(grad_sigmoid_manual(2.0))"]},{"cell_type":"markdown","metadata":{"id":"M-CXjUbhnS1R"},"source":["Also, note that you can compute higher-order derivatives by nesting tapes:"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1677549692908,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"5n8yDLgmnS1R","outputId":"7226e4d0-058f-4e4b-a95c-60294b42972a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n","tf.Tensor([0.31622776 0.44721356], shape=(2,), dtype=float32)\n","tf.Tensor([0.1423025  0.11925695], shape=(2,), dtype=float32)\n"]}],"source":["print(a)\n","with tf.GradientTape() as outer_tape:\n","    with tf.GradientTape() as tape:\n","        c = tf.sqrt(tf.square(a) + tf.square(b))\n","        dc_da = tape.gradient(c, a)\n","        print(dc_da)\n","    d2c_da2 = outer_tape.gradient(dc_da, a)\n","    print(d2c_da2)\n"]},{"cell_type":"markdown","metadata":{"id":"2g1nKB6P-OnA"},"source":["You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677549692908,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"7XaPRAwUyYms","outputId":"93e83035-0208-4630-dc94-05662af13755"},"outputs":[{"name":"stdout","output_type":"stream","text":["18.0\n","108.0\n"]}],"source":["x = tf.constant(3.0)\n","\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y\n","\n","# Use the tape to compute the gradient of z with respect to the\n","# intermediate value y.\n","# dz_dy = 2 * y and y = x ** 2 = 9\n","print(tape.gradient(z, y).numpy())\n","\n","# and then call again if you want to get the gradient with respect to x\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y\n","\n","print(tape.gradient(z, x).numpy())"]},{"cell_type":"markdown","metadata":{"id":"ISkXuY7YzIcS"},"source":["By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677549692908,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"zZaCm3-9zVCi","outputId":"2e39465a-97f8-4275-fc81-c06475d4e760"},"outputs":[{"name":"stdout","output_type":"stream","text":["[  4. 108.]\n","[2. 6.]\n"]}],"source":["x = tf.constant([1, 3.0])\n","with tf.GradientTape(persistent=True) as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y\n","\n","print(tape.gradient(z, x).numpy())  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n","print(tape.gradient(y, x).numpy())  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677549692909,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"j8bv_jQFg6CN"},"outputs":[],"source":["del tape   # Drop the reference to the tape"]},{"cell_type":"markdown","metadata":{"id":"JWKpEFwInS1W"},"source":["## The Functional API review\n","\n","To build deep learning models, we've seen so far how they can be composed functionally, like this (we call\n","it the \"Functional API\"):"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":453,"status":"ok","timestamp":1677549693359,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"kM5EbmnInS1W"},"outputs":[],"source":["# We use an `Input` object to describe the shape and dtype of the inputs.\n","# This is the deep learning equivalent of *declaring a type*.\n","# The shape argument is per-sample; it does not include the batch size.\n","# The functional API focused on defining per-sample transformations.\n","# The model we create will automatically batch the per-sample transformations,\n","# so that it can be called on batches of data.\n","inputs = tf.keras.Input(shape=(784,), dtype=\"float32\")\n","\n","# We call layers on these \"type\" objects\n","# and they return updated types (new shapes/dtypes).\n","x = tf.keras.layers.Dense(32, activation='relu')(inputs)\n","x = tf.keras.layers.Dropout(0.5)(x)\n","outputs = tf.keras.layers.Dense(10)(x)\n","\n","# A functional `Model` can be defined by specifying inputs and outputs.\n","# A model is itself a layer like any other.\n","model = tf.keras.Model(inputs, outputs)\n","\n","# Let's call our model on some data, for fun.\n","y = model(tf.ones((2, 784)))\n","assert y.shape == (2, 10)\n","\n","# You can pass a `training` argument in `__call__`\n","# (it will get passed down to the Dropout layer).\n","y = model(tf.ones((2, 784)), training=True)"]},{"cell_type":"markdown","metadata":{"id":"G-9DxZJ2Oxi9"},"source":["## Custom training loops\n"]},{"cell_type":"markdown","metadata":{"id":"jg_vtk83O4ui"},"source":["With gradient tapes, we can write our own training loops. This is particularly useful for more complex models, like GANs, where we need a much higher control over which weights are trained. "]},{"cell_type":"markdown","metadata":{"id":"hpdR_ONyxRVW"},"source":["First let us prepare the MNIST dataset."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":1620,"status":"ok","timestamp":1677549694976,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"j88aWsCgx4_5"},"outputs":[],"source":["# Prepare a dataset.\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","dataset = tf.data.Dataset.from_tensor_slices(\n","    (x_train.reshape(60000,784).astype(\"float32\") / 255, y_train)\n",")\n","dataset = dataset.shuffle(buffer_size=1024).batch(16)"]},{"cell_type":"markdown","metadata":{"id":"glUDoFzQx9JF"},"source":["Then we set up a loss function and so far, we have been using the `.compile()` and `.fit()` methods for training the model."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35487,"status":"ok","timestamp":1677549730459,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"sNzaHYivyhAG","outputId":"d2d157a5-c89e-4c88-c061-dfb06e9c0674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2023-02-28 20:58:43.451886: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n","2023-02-28 20:58:43.452378: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"]},{"name":"stdout","output_type":"stream","text":["3750/3750 [==============================] - 21s 5ms/step - loss: 0.8101 - acc: 0.7538\n","Epoch 2/2\n","3750/3750 [==============================] - 22s 6ms/step - loss: 0.4556 - acc: 0.8680\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x284347e50>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","model.compile(optimizer='sgd', loss=loss_fn, metrics=['acc'])\n","model.fit(dataset, epochs=2)"]},{"cell_type":"markdown","metadata":{"id":"JOLMaOw6yzcf"},"source":["We can replace these with a custom training loop like this:"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98834,"status":"ok","timestamp":1677549832597,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"2pkzysGznS1U","outputId":"6d03e775-e8c2-4b97-f9cd-46fc89c2d3f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Step: 0 Loss: 0.25871482491493225\n","Step: 100 Loss: 0.09273234009742737\n","Step: 200 Loss: 0.2915576994419098\n","Step: 300 Loss: 0.5641614198684692\n","Step: 400 Loss: 0.24844102561473846\n","Step: 500 Loss: 0.0930514708161354\n","Step: 600 Loss: 0.07068446278572083\n","Step: 700 Loss: 0.09715358167886734\n","Step: 800 Loss: 0.07388998568058014\n","Step: 900 Loss: 0.14864414930343628\n","Step: 1000 Loss: 0.32591161131858826\n","Step: 1100 Loss: 0.38956910371780396\n","Step: 1200 Loss: 0.5928006768226624\n","Step: 1300 Loss: 0.639584481716156\n","Step: 1400 Loss: 0.19506604969501495\n","Step: 1500 Loss: 0.0578753724694252\n","Step: 1600 Loss: 0.20680800080299377\n","Step: 1700 Loss: 0.3058369755744934\n","Step: 1800 Loss: 0.2530142664909363\n","Step: 1900 Loss: 0.28684407472610474\n","Step: 2000 Loss: 0.3055097162723541\n","Step: 2100 Loss: 0.5274989604949951\n","Step: 2200 Loss: 0.22680245339870453\n","Step: 2300 Loss: 0.4784732758998871\n","Step: 2400 Loss: 0.039486076682806015\n","Step: 2500 Loss: 0.19443649053573608\n","Step: 2600 Loss: 0.3305051922798157\n","Step: 2700 Loss: 0.35560914874076843\n","Step: 2800 Loss: 0.11124923825263977\n","Step: 2900 Loss: 0.3057097792625427\n","Step: 3000 Loss: 0.5513182282447815\n","Step: 3100 Loss: 0.11599723249673843\n","Step: 3200 Loss: 0.21865960955619812\n","Step: 3300 Loss: 0.15216870605945587\n","Step: 3400 Loss: 0.4926396310329437\n","Step: 3500 Loss: 0.1275358647108078\n","Step: 3600 Loss: 0.12029628455638885\n","Step: 3700 Loss: 0.060134388506412506\n","Step: 0 Loss: 0.06881184130907059\n","Step: 100 Loss: 0.27465692162513733\n","Step: 200 Loss: 0.1899089813232422\n","Step: 300 Loss: 0.22697266936302185\n","Step: 400 Loss: 0.13753937184810638\n","Step: 500 Loss: 0.5732684135437012\n","Step: 600 Loss: 0.10453023761510849\n","Step: 700 Loss: 0.19169938564300537\n","Step: 800 Loss: 0.03363090753555298\n","Step: 900 Loss: 0.2964850068092346\n","Step: 1000 Loss: 0.18499931693077087\n","Step: 1100 Loss: 0.4876055121421814\n","Step: 1200 Loss: 0.5925965309143066\n","Step: 1300 Loss: 0.12412849068641663\n","Step: 1400 Loss: 0.1928713619709015\n","Step: 1500 Loss: 0.521417498588562\n","Step: 1600 Loss: 0.2615467309951782\n","Step: 1700 Loss: 0.09820695221424103\n","Step: 1800 Loss: 0.1168602705001831\n","Step: 1900 Loss: 0.21615444123744965\n","Step: 2000 Loss: 0.1226636990904808\n","Step: 2100 Loss: 0.32666027545928955\n","Step: 2200 Loss: 0.28911277651786804\n","Step: 2300 Loss: 0.12546679377555847\n","Step: 2400 Loss: 0.6529440879821777\n","Step: 2500 Loss: 0.14038534462451935\n","Step: 2600 Loss: 0.27115398645401\n","Step: 2700 Loss: 0.19041801989078522\n","Step: 2800 Loss: 0.09395009279251099\n","Step: 2900 Loss: 0.3935794234275818\n","Step: 3000 Loss: 0.6465421319007874\n","Step: 3100 Loss: 0.9648412466049194\n","Step: 3200 Loss: 0.2972922921180725\n","Step: 3300 Loss: 0.354938805103302\n","Step: 3400 Loss: 0.18138381838798523\n","Step: 3500 Loss: 0.30296385288238525\n","Step: 3600 Loss: 0.13571999967098236\n","Step: 3700 Loss: 0.3461433947086334\n"]}],"source":["# Let's demonstrate how to use a training loop.\n","\n","# Loss and optimizer.\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n","\n","for epochs in range(2):\n","  for step, (x, y) in enumerate(dataset):\n","      with tf.GradientTape() as tape:\n","          # Forward pass.\n","          logits = model(x)\n","          # External loss value for this batch.\n","          loss = loss_fn(y, logits)\n","          # Get gradients of the loss wrt the weights.\n","          gradients = tape.gradient(loss, model.trainable_weights)\n","\n","      # Update the weights of our linear layer.\n","      optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n","\n","      # Logging\n","      if step % 100 == 0:\n","          print(\"Step:\", step, \"Loss:\", float(loss))"]},{"cell_type":"markdown","metadata":{"id":"6_PMHrizRQBm"},"source":["For more information also check out the guides from tensorflow website: [1](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough#tensorflow_programming) and [2](https://www.tensorflow.org/guide/basic_training_loops). "]},{"cell_type":"markdown","metadata":{"id":"FhvHffy9TJDQ"},"source":["## Keeping track of performance metrics"]},{"cell_type":"markdown","metadata":{"id":"08C37uQ3HDvF"},"source":["### With `fit()` method\n","\n","As you're training a model, you want to keep track of metrics such as classification\n","accuracy, precision, recall, AUC, etc. Besides, you want to monitor these metrics not\n"," only on the training data, but also on a validation set.\n","\n","**Monitoring metrics**\n","\n","You can pass a list of metric objects to `compile()`, like this:\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11281,"status":"ok","timestamp":1677549850008,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"YOVTAwbVHDvG","outputId":"890657a9-e62e-477a-aa35-7e7cdec579dd"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-02-28 21:00:09.105260: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"]},{"name":"stdout","output_type":"stream","text":["3750/3750 [==============================] - 24s 6ms/step - loss: 2.1451 - acc: 0.5986\n"]}],"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",")\n","history = model.fit(dataset, epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"7YO6gEfeHDvG"},"source":["**Passing validation data to `fit()`**\n","\n","You can pass validation data to `fit()` to monitor your validation loss & validation\n"," metrics. Validation metrics get reported at the end of each epoch."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10638,"status":"ok","timestamp":1677549863255,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"WuEWeh79HDvG","outputId":"622fed12-7e2e-4f7c-c3c4-2ba6cf7fe2bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["3749/3750 [============================>.] - ETA: 0s - loss: 2.2408 - acc: 0.5027"]},{"name":"stderr","output_type":"stream","text":["2023-02-28 21:00:57.731739: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"]},{"name":"stdout","output_type":"stream","text":["3750/3750 [==============================] - 26s 7ms/step - loss: 2.2408 - acc: 0.5027 - val_loss: 2.3026 - val_acc: 0.6331\n"]}],"source":["val_dataset = tf.data.Dataset.from_tensor_slices(\n","    (x_test.reshape(10000,784).astype(\"float32\") / 255, y_test)\n",") # again we hid a bug here. were you able to correct it?\n","val_dataset = val_dataset.shuffle(buffer_size=1024).batch(16)    \n","\n","history = model.fit(dataset, epochs=1, validation_data=val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"-KDRgDMfnS1U"},"source":["### With custom loops\n","\n","Keras offers a broad range of built-in metrics, like `tf.keras.metrics.AUC`\n","or `tf.keras.metrics.PrecisionAtRecall`. It's also easy to create your\n","own metrics in a few lines of code.\n","\n","To use a metric in a custom training loop, you would:\n","\n","- Instantiate the metric object, e.g. `metric = tf.keras.metrics.AUC()`\n","- Call its `metric.udpate_state(targets, predictions)` method for each batch of data\n","- Query its result via `metric.result()`\n","- Reset the metric's state at the end of an epoch or at the start of an evaluation via\n","`metric.reset_state()`\n","\n","Here's a simple example:"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283997,"status":"ok","timestamp":1677550162057,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"utnsPHb-nS1U","outputId":"7527a0a5-6408-4ac3-e980-b07785e35770"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 Step: 0\n","Total running accuracy so far: 0.688\n","Epoch: 0 Step: 200\n","Total running accuracy so far: 0.890\n","Epoch: 0 Step: 400\n","Total running accuracy so far: 0.905\n","Epoch: 0 Step: 600\n","Total running accuracy so far: 0.905\n","Epoch: 0 Step: 800\n","Total running accuracy so far: 0.910\n","Epoch: 0 Step: 1000\n","Total running accuracy so far: 0.909\n","Epoch: 0 Step: 1200\n","Total running accuracy so far: 0.912\n","Epoch: 0 Step: 1400\n","Total running accuracy so far: 0.915\n","Epoch: 0 Step: 1600\n","Total running accuracy so far: 0.916\n","Epoch: 0 Step: 1800\n","Total running accuracy so far: 0.918\n","Epoch: 0 Step: 2000\n","Total running accuracy so far: 0.918\n","Epoch: 0 Step: 2200\n","Total running accuracy so far: 0.920\n","Epoch: 0 Step: 2400\n","Total running accuracy so far: 0.921\n","Epoch: 0 Step: 2600\n","Total running accuracy so far: 0.922\n","Epoch: 0 Step: 2800\n","Total running accuracy so far: 0.922\n","Epoch: 0 Step: 3000\n","Total running accuracy so far: 0.923\n","Epoch: 0 Step: 3200\n","Total running accuracy so far: 0.924\n","Epoch: 0 Step: 3400\n","Total running accuracy so far: 0.925\n","Epoch: 0 Step: 3600\n","Total running accuracy so far: 0.926\n","Epoch: 1 Step: 0\n","Total running accuracy so far: 0.875\n","Epoch: 1 Step: 200\n","Total running accuracy so far: 0.945\n","Epoch: 1 Step: 400\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 600\n","Total running accuracy so far: 0.948\n","Epoch: 1 Step: 800\n","Total running accuracy so far: 0.947\n","Epoch: 1 Step: 1000\n","Total running accuracy so far: 0.946\n","Epoch: 1 Step: 1200\n","Total running accuracy so far: 0.948\n","Epoch: 1 Step: 1400\n","Total running accuracy so far: 0.948\n","Epoch: 1 Step: 1600\n","Total running accuracy so far: 0.949\n","Epoch: 1 Step: 1800\n","Total running accuracy so far: 0.949\n","Epoch: 1 Step: 2000\n","Total running accuracy so far: 0.949\n","Epoch: 1 Step: 2200\n","Total running accuracy so far: 0.949\n","Epoch: 1 Step: 2400\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 2600\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 2800\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 3000\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 3200\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 3400\n","Total running accuracy so far: 0.950\n","Epoch: 1 Step: 3600\n","Total running accuracy so far: 0.951\n"]}],"source":["# Instantiate a metric object\n","accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","\n","# Prepare our loss, and optimizer.\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","\n","for epoch in range(2):\n","    # Iterate over the batches of a dataset.\n","    for step, (x, y) in enumerate(dataset):\n","        with tf.GradientTape() as tape:\n","            logits = model(x)\n","            # Compute the loss value for this batch.\n","            loss_value = loss_fn(y, logits)\n","\n","        # Update the state of the `accuracy` metric.\n","        accuracy.update_state(y, logits)\n","\n","        # Update the weights of the model to minimize the loss value.\n","        gradients = tape.gradient(loss_value, model.trainable_weights)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n","\n","        # Logging the current accuracy value so far.\n","        if step % 200 == 0:\n","            print(\"Epoch:\", epoch, \"Step:\", step)\n","            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n","\n","    # Reset the metric's state at the end of an epoch\n","    accuracy.reset_state()"]},{"cell_type":"markdown","metadata":{"id":"ngNTwkcTHDvG"},"source":["## Using callbacks for checkpointing (and more)\n","\n","If training goes on for more than a few minutes, it's important to save your model at\n"," regular intervals during training. You can then use your saved models\n","to restart training in case your training process crashes (this is important for\n","multi-worker distributed training, since with many workers at least one of them is\n"," bound to fail at some point).\n","\n","An important feature of Keras is **callbacks**, configured in `fit()`. Callbacks are\n"," objects that get called by the model at different point during training, in particular:\n","\n","- At the beginning and end of each batch\n","- At the beginning and end of each epoch\n","\n","Callbacks are a way to make model trainable entirely scriptable.\n","\n","You can use callbacks to periodically save your model. Here's a simple example: a\n"," `ModelCheckpoint` callback\n","configured to save the model at the end of every epoch. The filename will include the\n"," current epoch.\n","\n","```python\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath='path/to/my/model_{epoch}',\n","        save_freq='epoch')\n","]\n","model.fit(dataset, epochs=2, callbacks=callbacks)\n","```"]},{"cell_type":"markdown","metadata":{"id":"TU9ST3PjHDvG"},"source":["You can also use callbacks to do things like periodically changing the learning of your\n","optimizer, streaming metrics to a Slack bot, sending yourself an email notification\n"," when training is complete, etc.\n","\n","For detailed overview of what callbacks are available and how to write your own, see the\n","[guide to writing custom callbacks](https://keras.io/guides/writing_your_own_callbacks/)."]},{"cell_type":"markdown","metadata":{"id":"998g0RfgUxhO"},"source":["#⏰ Exercise: using autodiff to calculate square roots\n","We have a very powerful tool in autodiff so let us use it on a problem that you can solve using only pen and paper.\n","To find the square root of a number $a$, we can solve the equation:\n","$$ f(x) = x^2-a = 0$$\n","To approximate the roots of $f(x)$, we can start with a guess $x_n$. Then, our next guess should be:\n","$$ x_{n+1} = x_n - \\frac{f(x)}{f'(x)}$$\n","And we repeat this process until our guesses do not change very much.\n","Now, we will implement this with autodiff."]},{"cell_type":"markdown","metadata":{"id":"pPAnabGlhk6T"},"source":["Let's say we want to calculate the square root of a number `a`. First initialize a `Variable` with a random value."]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1677550170359,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"5jbCPgA9h1S5"},"outputs":[],"source":["a = tf.constant(26.0, name='a') # pick a number and make it a tf Constant (make sure it is a float)\n","x = tf.Variable(tf.random.normal(shape=(1,1)), name='x') # tf Variable that is randomly initialized"]},{"cell_type":"markdown","metadata":{"id":"iv0Hl9oJh_jk"},"source":["Next, write an expression in tensorflow for $f(x)$"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":161,"status":"ok","timestamp":1677550171677,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"06V1yzVxiSrE"},"outputs":[],"source":["f = tf.square(x) - a # f(x)"]},{"cell_type":"markdown","metadata":{"id":"9IuQc0SviVI6"},"source":["Now use this expression inside a gradient tape to calculate its derivative and find the updated guess. Remeber to update a tf Variable, you should call the `assign` method."]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677550172877,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"X2fEDoFVicTY","outputId":"9ed130b1-7898-422d-9ddb-1d538265ff89"},"outputs":[{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(1, 1) dtype=float32, numpy=array([[17.562355]], dtype=float32)>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["with tf.GradientTape() as tape:\n","    # expression for f(x)\n","    f = tf.square(x) - a\n","    # derivative of f(x)\n","    grad = tape.gradient(f, x)\n","# write the update rule using f(x) and f'(x)\n","x.assign_sub(f/grad)"]},{"cell_type":"markdown","metadata":{"id":"5zd9o6svics8"},"source":["Put is all together and run this in a loop for a number of iterations, and print the guess after each iteration."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173,"status":"ok","timestamp":1677550280841,"user":{"displayName":"Shen Juin Lee","userId":"01524636780029761489"},"user_tz":300},"id":"pN0Qsdjpj2_U","outputId":"f7aa1522-00a7-4e28-d251-aeaab7b60770"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-7.195348]]\n","[[-5.404397]]\n","[[-5.1076474]]\n","[[-5.0990267]]\n","[[-5.0990195]]\n","[[-5.0990195]]\n","[[-5.0990195]]\n","[[-5.0990195]]\n","[[-5.0990195]]\n","[[-5.0990195]]\n"]}],"source":["a = tf.constant(26.0, name='a') # pick a number and make it a tf Constant (make sure it is a float)\n","x = tf.Variable(tf.random.normal(shape=(1,1)), name='x') # tf Variable that is randomly initialized\n","f = tf.square(x) - a # f(x)\n","\n","for i in range(10):\n","    with tf.GradientTape() as tape:\n","        # expression for f(x)\n","        f = tf.square(x) - a\n","        # derivative of f(x)\n","        grad = tape.gradient(f, x)\n","    # write the update rule using f(x) and f'(x)\n","    x.assign_sub(f/grad)\n","    # print guess\n","    print(x.numpy())"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"676061f514cc8f19bd2e695b1824ddc1c199ec03dc6eadf4aa0b9e6cc1e4dfd0"}}},"nbformat":4,"nbformat_minor":0}
